2019_08_21일 22시 회의

1. aihub dataset 분석 + 파이프라인 구상

- 구축 데이터 총 러닝 타임 : 21시간 10분 36초
- 동영상클립
 최소 러닝타임 : 1분 49초
 최대 러닝타임 : 3분 47초
- 총 클립수 : 1,943건
- 총 이미지 수 : 158,383장
- 대화 정보 : 10,961 pa

http://aihub.or.kr/sites/default/files/%E1%84%86%E1%85%A5%E1%86%AF%E1%84%90%E1%85%B5%E1%84%86%E1%85%A9%E1%84%83%E1%85%A1%E1%86%AF_%E1%84%80%E1%85%AE%E1%84%8C%E1%85%A9%E1%84%83%E1%85%A9.png
감정정보가 명확하지 않은 데이터? 지만 end to end model을 만들자! 



annotation 예시
<box frame="133" keyframe="0" occluded="0" outside="0" xbr="1390.32" xtl="836.65" ybr="959.74" ytl="129.55">
      <attribute name="image_emotion">neu</attribute>
      <attribute name="image_arousal">5</attribute>
      <attribute name="image_valence">5</attribute>
      <attribute name="multi_emotion">default</attribute>
      <attribute name="multi_arousal">0</attribute>
      <attribute name="multi_valence">0</attribute>
      <attribute name="position1">ahead_behind</attribute>
      <attribute name="position2">default</attribute>
      <attribute name="position3">default</attribute>
      <attribute name="position4">default</attribute>
      <attribute name="position5">default</attribute>
      <attribute name="position_id1">2</attribute>
      <attribute name="position_id2">default</attribute>
      <attribute name="position_id3">default</attribute>
      <attribute name="position_id4">default</attribute>
      <attribute name="position_id5">default</attribute>
      <attribute name="predicate1">sit</attribute>
      <attribute name="predicate2">catch</attribute>
      <attribute name="predicate3">default</attribute>
      <attribute name="predicate4">default</attribute>
      <attribute name="predicate5">default</attribute>
      <attribute name="predicate_id1">default</attribute>
      <attribute name="predicate_id2">default</attribute>
      <attribute name="predicate_id3">default</attribute>
      <attribute name="predicate_id4">default</attribute>
      <attribute name="predicate_id5">default</attribute>
      <attribute name="person_id">2</attribute>
      <attribute name="object_id">1</attribute>
      <attribute name="script_tagging">no</attribute>
    </box>
    

# 프로젝트 구조 

convert.py/.sh 영상 -> 오디오  |  json 에서 발화시작, 종료시간별로 오디오 클립 분할 + bad data제거 (bad sampled)

extract.py     json -> txt    |  오디오 클립path와 image_emotion, image_arousal, image_valence, multi_emotion, multi_arousal, multi_valence있는 메타데이터 생성

model.py       오디오 -> 감정  | 분류모델  

audio_utils.py 간단한 오디오 처리
    def data_load(audio_f, sr=22050, file_format="wav", num_channels=1):
      audio_binary = tf.read_file(audio_f)
      y = tf.contrib.ffmpeg.decode_audio(audio_binary, file_format, sr, num_channels)
      return tf.squeeze(y, 1), sr

      y, sr = data_load(audio_f, sr, file_format, num_channels)
datafeeder.py    

2. 타임라인

3. 아이데이션 
https://docs.google.com/spreadsheets/d/1BiB9gGgDNZwUAsPT9zhaf1tMyx9gpnyKl4PBgdUfsIs/edit#gid=1428349732

4. 기타 사항 
네이버 음성인식 해커톤 https://campaign.naver.com/aihackathon_speech/
